# -*- coding: utf-8 -*-
"""TD-nstep_off_policy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rxpmsd9Q-No82aytcOYWfwaj1O3s6y2_

## Configurações Iniciais
"""

from IPython.display import clear_output
import sys

IN_COLAB = 'google.colab' in sys.modules

if IN_COLAB:
    # for saving videos
    !apt-get install ffmpeg

    !pip install gymnasium moviepy
    !pip install optuna

    # clone repository
    !git clone https://github.com/pablo-sampaio/rl_facil
    sys.path.append("/content/rl_facil")

else:
    from os import path
    sys.path.append( path.dirname( path.dirname( path.abspath("__main__") ) ) )

#clear_output()

import gymnasium as gym
from gymnasium.wrappers import TimeLimit
from collections import deque

import numpy as np

def epsilon_greedy(Q, state, epsilon):
    if np.random.rand() < epsilon:
        return np.random.randint(Q.shape[1])
    else:
        return np.argmax(Q[state])

def run_nstep_td(env, episodes, nsteps=1, lr=0.1, gamma=0.95, epsilon=0.1, verbose=False):
    assert isinstance(env.observation_space, gym.spaces.Discrete)
    assert isinstance(env.action_space, gym.spaces.Discrete)
    assert isinstance(nsteps, int)

    num_actions = env.action_space.n

    # inicializa a tabela Q com valores aleatórios pequenos (para evitar empates)
    Q = np.random.uniform(low=-0.01, high=+0.01, size=(env.observation_space.n, num_actions))

    gamma_array = np.array([ gamma**i for i in range(0,nsteps)])
    gamma_power_nstep = gamma**nsteps

    # para cada episódio, guarda sua soma de recompensas (retorno não-descontado)
    sum_rewards_per_ep = []

    # loop principal
    for i in range(episodes):
        done = False
        sum_rewards, reward = 0, 0

        state, _ = env.reset()
        # escolhe a próxima ação
        action = epsilon_greedy(Q, state, epsilon)

        # históricos de: estados, ações e recompensas
        hs = deque(maxlen=nsteps)
        ha = deque(maxlen=nsteps)
        hr = deque(maxlen=nsteps)

        # executa 1 episódio completo, fazendo atualizações na Q-table
        while not done:
            # realiza a ação
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            sum_rewards += reward

            # escolhe (antecipadamente) a ação do próximo estado
            next_action = epsilon_greedy(Q, next_state, epsilon)

            hs.append(state)
            ha.append(action)
            hr.append(reward)

            # se o histórico estiver completo com 'n' passos
            # vai fazer uma atualização no valor Q do estado mais antigo
            if len(hs) == nsteps:
                if terminated:
                    # para estados terminais
                    V_next_state = 0
                else:
                    # para estados não-terminais -- valor da próxima ação (já escolhida)
                    V_next_state = np.max(Q[next_state])

                # delta = (estimativa usando a nova recompensa) - estimativa antiga
                delta = ( sum(gamma_array * hr) + gamma_power_nstep * V_next_state ) - Q[hs[0],ha[0]]

                # atualiza a Q-table para o par (estado,ação) de n passos atrás
                Q[hs[0],ha[0]] += lr * delta

            # preparação para avançar mais um passo
            # lembrar que a ação a ser realizada já está escolhida
            state = next_state
            action = next_action
            # fim do laço por episódio

        # ao fim do episódio, atualiza o Q dos estados que restaram no histórico

        # é igual ao V_next_state, exceto em episódios muito curtos (com duração menor que "nsteps")
        V_end_state = 0 if terminated else np.max(Q[next_state])

        # inferior ao "nsteps" apenas em episódios muito curtos
        steps_to_end = min(nsteps, len(hs))
        for j in range(steps_to_end-1,0,-1):
            hs.popleft()
            ha.popleft()
            hr.popleft()
            delta = ( sum(gamma_array[0:j]*hr) + gamma_array[j]*V_end_state ) - Q[hs[0],ha[0]]
            Q[hs[0],ha[0]] += lr * delta

        sum_rewards_per_ep.append(sum_rewards)

        # a cada 100 episódios, imprime informação sobre o progresso
        if verbose and ((i+1) % 100 == 0):
            avg_reward = np.mean(sum_rewards_per_ep[-100:])
            print(f"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}")

    return sum_rewards_per_ep, Q


from envs import RacetrackEnv
from envs.wrappers import ObservationDiscretizerWrapper

from util.experiments import repeated_exec
from util.plot import plot_result, plot_multiple_results
from util.notebook import display_videos_from_path
from util.qtable_helper import evaluate_qtable_policy, record_video_qtable

"""## 1 - TD Learning Off-Policy de n Passos

O TD Learning Off-Policy de n passos é uma extensão do TD Learning que utiliza uma sequência de *n* passos para atualizar a estimativa do $Q(s,a)$.

### Relembrando o Off-Policy
No aprendizado off-policy, a política de comportamento (que escolhe as ações) é diferente da política alvo (que é avaliada). Um exemplo clássico é o Q-Learning.

### TD Learning On-Policy
No TD Learning on-policy, a mesma política é usada tanto para escolher as ações quanto para avaliar a política. Um exemplo é o SARSA.

### Correção no TD Learning Off-Policy
A correção no TD Learning off-policy é feita através de métodos como o Ordinary Importance Sampling e o Weighted Importance Sampling.

### Exemplos Simples
Vamos ver exemplos de como essa correção é calculada em episódios pequenos, de 1 até 3 passos.

#### Ordinary Importance Sampling
Para **$n=1$**:
- experiência: $s, a, r_1, s_1, a_1$
- estimativa: $Q_{target} = r_1 + \gamma . Q(s_1,a_1) * \frac{\pi(a_1|s_1)}{b(a_1|s_1)}$

#### Weighted Importance Sampling
Para **$n=2$**:
- experiência: $s, a, r_1, s_1, a_1, r_2, s_2, a_2$
- estimativa: $Q_{target} = r_1 + \gamma .r_2 + \gamma^2 . Q(s_2,a_2) * \frac{\pi(a_1|s_1)\pi(a_2|s_2)}{b(a_1|s_1)b(a_2|s_2)}$

### Implementação
Vamos implementar as variantes do off-policy:
- Ordinary Importance Sampling
- Weighted Importance Sampling
- Pesos Truncados
"""

# para ambientes gymnasium
#ENV_NAME, r_max = "Taxi-v3", 10
#ENV_NAME, r_max = "CliffWalking-v0", 0
#ENV_NAME, r_max = "FrozenLake-v1", 0
ENV_NAME, r_max = "RaceTrack-v0", 0

env = gym.make(ENV_NAME)

# para ambientes instanciados diretamente
# atenção: vale a pena aplicar um TimeLimit
#env = RacetrackEnv()
#r_max = 0

EPISODES = 5_000
LR = 0.1
GAMMA = 0.95
EPSILON = 0.1
NSTEPS = 5

# Roda o algoritmo "n-step TD"
rewards1, qtable1 = run_nstep_td(env, EPISODES, NSTEPS, LR, GAMMA, EPSILON, verbose=True)
print("Últimos resultados: media =", np.mean(rewards1[-20:]), ", desvio padrao =", np.std(rewards1[-20:]))

# Mostra um gráfico de episódios x retornos não descontados
plot_result(rewards1, r_max, x_log_scale=True)

evaluate_qtable_policy(env, qtable1, 10, epsilon=0.1, verbose=True);

# atenção: precisa passar a ID do ambiente no gymnasium ou instanciar diretamente um novo ambiente com o render_mode "rgb_array"
record_video_qtable(ENV_NAME, qtable1, episodes=3, folder='videos/', prefix='nstep-td')
#record_video_qtable(RacetrackEnv(render_mode="rgb_array"), qtable1, episodes=3, folder='videos/', prefix='nstep-td')

display_videos_from_path('./videos', prefix='nstep-td')

"""A seguir, vamos rodar alguns experimentos variando a quantidade de passos:"""

RUNS = 3
results1 = []
for nstep in [1, 2, 3]:
    results1.append( repeated_exec(RUNS, f"{nstep}-step TD (LR={LR})", run_nstep_td, env, EPISODES, nstep, LR) )
    clear_output()

plot_multiple_results(results1, window=30, x_log_scale=True)

"""# Agora com o ambiente Cliffwalking"""

import numpy as np

ENV_NAME, r_max = "CliffWalking-v0", 0

env = gym.make(ENV_NAME)

EPISODES = 5_000
LR = 0.1
GAMMA = 0.95
EPSILON = 0.1
NSTEPS = 5

# Roda o algoritmo "n-step TD" no ambiente CliffWalking
rewards1, qtable1 = run_nstep_td(env, EPISODES, NSTEPS, LR, GAMMA, EPSILON, verbose=True)
print("Últimos resultados: media =", np.mean(rewards1[-20:]), ", desvio padrao =", np.std(rewards1[-20:]))
# Mostra um gráfico de episódios x retornos não descontados
plot_result(rewards1, r_max, None)
evaluate_qtable_policy(env, qtable1, 10, epsilon=0.1, verbose=True);
record_video_qtable(ENV_NAME, qtable1, episodes=3, folder='videos/', prefix='nstep-td-cliffwalking')
display_videos_from_path('./videos', prefix='nstep-td-cliffwalking')

# Experimentos variando a quantidade de passos no CliffWalking
RUNS = 3
results1 = []
for nstep in [1, 2, 3]:
    results1.append(repeated_exec(RUNS, f"{nstep}-step TD (LR={LR})", run_nstep_td, env, EPISODES, nstep, LR))
    clear_output()
plot_multiple_results(results1, window=30, x_log_scale=True)

"""## 2 - Lidando com Estados Contínuos

Vamos usar os mesmos algoritmos de antes, baseados em Q-Table, para lidar com ambientes de estados contínuos.

Para isso, vamos usar um *wrapper* que discretiza os estados desses ambientes.

Primeiramente, vamos analisar, abaixo, o espaço de estados de um ambiente contínuo:
"""

ENV_NAME = "CartPole-v1"
#ENV_NAME, r_max = "Taxi-v3" *** LEMBRAR DE COMENTAR OS BINS E env2b -> env2a
#ENV_NAME, r_max = "CliffWalking-v0"
#ENV_NAME, r_max = "FrozenLake-v1"
r_max_plot = 200

env2a = gym.make(ENV_NAME)

# vamos ver como é um estado deste ambiente?
print("Espaço de estados/observações: ", env2a.observation_space)
print("  - formato: ", env2a.observation_space.shape)
print("  - exemplo: ", env2a.reset())

"""Abaixo, nós encapsulamos o ambiente contínuo em um wrapper para discretizá-lo.

Os parâmetros indicam quantos valores discretos foram usados para representar cada uma das dimensões do estado.

Como resultado, o espaço de estados torna-se do tipo `Discrete`, o que indica que cada "estado" é representado como um único número inteiro.
"""

# Encapsula o ambiente em nosso wrapper
# atenção para o parâmetro BINS: deve ter um valor para cada componente do estado
BINS = [5, 30, 30, 30]
env2b = ObservationDiscretizerWrapper(env2a, BINS)

env2b.observation_space

"""Agora, podemos rodar treinamentos com quaisquer dos algoritmos que temos visto. Vamos rodar o *TD Learning de n passos*:"""

EPISODES = 5_000
LR = 0.2
GAMMA = 0.95
EPSILON = 0.1
NSTEPS = 4

rewards2, qtable2 = run_nstep_td(env2b, EPISODES, NSTEPS, LR, GAMMA, EPSILON, verbose=True)

print("Últimos resultados: media =", np.mean(rewards2[-20:]), ", desvio padrao =", np.std(rewards2[-20:]))

# Gera um gráfico de episódios x retornos (não descontados)
plot_result(rewards2, r_max_plot)

# Faz alguns testes, usando a tabela de forma greedy
evaluate_qtable_policy(env2b, qtable2, 10, 0.0, verbose=True)

# Salva vídeo
# Atenção: é recomendado criar nova instância do ambiente e do wrapper!
env_test = gym.make(ENV_NAME, render_mode="rgb_array")
env_test = ObservationDiscretizerWrapper(env_test, BINS)
record_video_qtable(env_test, qtable2, episodes=3, folder='videos/', prefix='nstep-cartpole')

display_videos_from_path('./videos', prefix='nstep-cartpole')

"""## 3 - Otimizando Parâmetros

Vamos usar a biblioteca *Optuna* para otimizar (hiper-)parâmetros dos algoritmos de treinamento.
"""

import optuna

"""### 3.1 - Ambiente Discreto

Este é o caso mais simples, porque não precisamos aplicar nenhum wrapper.

Primeiro, você precisa fazer uma função que receber um parâmetro do tipo `Trial` (definido no optuna) e retorna uma medida de desempenho.

Dentro da função, você usa o objeto *trial* para pedir "sugestões" de valores para os hiper-parâmetros do seu algoritmo.
"""

def train_nstep_td_racetrack(trial : optuna.Trial):
    # chama os métodos do "trial" (tentativa) para sugerir valores para os parâmetros
    eps    = trial.suggest_float('epsilon', 0.01, 0.2)
    gamma  = trial.suggest_float('gamma', 0.90, 1.00)
    lr     = trial.suggest_float('lr', 0.001, 1.0, log=True) # sugere na escala log (maior chance de escolher valor menor)
    nsteps = trial.suggest_int('nsteps', 1, 16)

    # outra opção trial.suggest_categorical('param', ['value1', 'value2'])

    print(f"\nTRIAL #{trial.number}: {trial.params}")

    # cria o ambiente Racetrack, mas insere-o em um wrapper para limitar o tamanho do episódio
    env = gym.make("RaceTrack-v0")

    # Roda o algoritmo "n-step TD"
    rewards, qtable = run_nstep_td(env, EPISODES, nsteps, lr, gamma, eps, verbose=False)

    # Retorna a média dos últimos 20 episódios
    return np.mean(rewards[-20:])

study = optuna.create_study(direction='maximize')
study.optimize(train_nstep_td_racetrack, n_trials=3)

print("Melhores parâmetros:")
print(study.best_params)

import numpy as np
def train_nstep_td_cliffwalking(trial : optuna.Trial):
    # chama os métodos do "trial" (tentativa) para sugerir valores para os parâmetros
    eps    = trial.suggest_float('epsilon', 0.01, 0.2)
    gamma  = trial.suggest_float('gamma', 0.90, 1.00)
    lr     = trial.suggest_float('lr', 0.001, 1.0, log=True) # sugere na escala log (maior chance de escolher valor menor)
    nsteps = trial.suggest_int('nsteps', 1, 16)

    print(f"\nTRIAL #{trial.number}: {trial.params}")

    # cria o ambiente CliffWalking-v0
    env = gym.make("CliffWalking-v0")

    # Roda o algoritmo "n-step TD"
    rewards, qtable = run_nstep_td(env, EPISODES, nsteps, lr, gamma, eps, verbose=False)

    # Retorna a média dos últimos 20 episódios
    return np.mean(rewards[-20:])

study = optuna.create_study(direction='maximize')
study.optimize(train_nstep_td_cliffwalking, n_trials=3)

print("Melhores parâmetros:")
study.best_params

"""### 3.2 - Ambiente Contínuo

Para ambientes contínuos, precisamos encapsular o ambiente em um wrapper para discretizar os estados.

Primeiro, você precisa fazer uma função que receber um parâmetro do tipo `Trial` (definido no optuna) e retorna uma medida de desempenho.

Dentro da função, você usa o objeto *trial* para pedir "sugestões" de valores para os hiper-parâmetros do seu algoritmo.
"""

def train_nstep_td_cartpole(trial : optuna.Trial):
    # chama os métodos do "trial" (tentativa) para sugerir valores para os parâmetros
    eps    = trial.suggest_float('epsilon', 0.01, 0.2)
    gamma  = trial.suggest_float('gamma', 0.90, 1.00)
    lr     = trial.suggest_float('lr', 0.001, 1.0, log=True) # sugere na escala log (maior chance de escolher valor menor)
    nsteps = trial.suggest_int('nsteps', 1, 16)

    # outra opção trial.suggest_categorical('param', ['value1', 'value2'])

    print(f"\nTRIAL #{trial.number}: {trial.params}")

    # cria o ambiente CartPole, mas insere-o em um wrapper para discretizar os estados
    env = gym.make("CartPole-v1")
    BINS = [5, 30, 30, 30]
    env = ObservationDiscretizerWrapper(env, BINS)

    # Roda o algoritmo "n-step TD"
    rewards, qtable = run_nstep_td(env, EPISODES, nsteps, lr, gamma, eps, verbose=False)

    # Retorna a média dos últimos 20 episódios
    return np.mean(rewards[-20:])

study = optuna.create_study(direction='maximize')
study.optimize(train_nstep_td_cartpole, n_trials=3)

print("Melhores parâmetros:")
print(study.best_params)

"""## 4 - Experimentos Completos

Agora que você descobriu bons parâmetros, que tal rodar um treinamento mais longo com o seu algoritmo?

Para cada ambiente, comparar os parâmetros default com os parâmetros otimizados.

### Racetrack
"""

#env_race = TimeLimit(RacetrackEnv(), 500)
env_race = gym.make("RaceTrack-v0")
NUM_EPISODES = 12_000
RUNS = 3

results = []

results.append( repeated_exec(RUNS, f"TD n-passos (default)", run_nstep_td, env_race, NUM_EPISODES) )
#clear_output()

results.append( repeated_exec(RUNS, f"TD n-passos (otimizado)", run_nstep_td, env_race, NUM_EPISODES, **study.best_params) )
clear_output()

plot_multiple_results(results, x_log_scale=False)

"""### CartPole"""

env_cart = gym.make("CartPole-v1")
NUM_EPISODES = 12_000
RUNS = 3

results = []

wrapped_env_cart = ObservationDiscretizerWrapper(env_cart, [30,30,30,30])
results.append( repeated_exec(RUNS, f"TD n-passos (default)", run_nstep_td, wrapped_env_cart, NUM_EPISODES, auto_load=True) )
#clear_output()

params = study.best_params

bins1 = [30, 30, 30, 30]

wrapped_env_cart = ObservationDiscretizerWrapper(env_cart, bins1)
results.append( repeated_exec(RUNS, f"TD n-passos (otimizado)", run_nstep_td, wrapped_env_cart, NUM_EPISODES, epsilon=params['epsilon'], lr=params['lr'], nsteps=params['nsteps'], auto_load=True) )
clear_output()

plot_multiple_results(results, x_log_scale=False)

"""# Cliffwalking"""

env_cliff = gym.make("CliffWalking-v0")
NUM_EPISODES = 12_000
RUNS = 3

results = []

results.append(repeated_exec(RUNS, f"TD n-passos (default)", run_nstep_td, env_cliff, NUM_EPISODES))
#clear_output()

results.append(repeated_exec(RUNS, f"TD n-passos (otimizado)", run_nstep_td, env_cliff, NUM_EPISODES, **study.best_params))
clear_output()

plot_multiple_results(results, x_log_scale=True)

"""# Observações finais

Como podemos observar nos resultados obtidos nesses experimentos com o TD n step off-policy, esse tipo de algoritmo tem bons retornos em ambientes de teste discretos, porém não tem muitos ganhos de aprendizado em ambientes contínuos
"""